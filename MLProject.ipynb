{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(url=\"https://raw.githubusercontent.com/tuclaure/CSU-CS-345/refs/heads/main/Data/bank-additional/bank-additional/bank-additional.csv\"):\n",
    "    response = requests.get(url)\n",
    "    data = StringIO(response.text)\n",
    "    df = pd.read_csv(data, sep=\";\")\n",
    "    return df\n",
    "\n",
    "def map_columns(df, columns, mapping):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].map(mapping)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_columns(df, columns):\n",
    "    # Check if the specified columns exist in the dataframe\n",
    "    missing_cols = [col for col in columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"The following columns are not in the DataFrame: {', '.join(missing_cols)}\")\n",
    "    \n",
    "    # Apply one-hot encoding to the specified columns\n",
    "    df_encoded = pd.get_dummies(df, columns=columns, drop_first=False)\n",
    "\n",
    "    df_encoded = df_encoded.map(lambda x: 1 if x is True else (0 if x is False else x))\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def encode_cyclic_information(df, column_name, mapping):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Map the days to numbers\n",
    "    df[column_name] = df[column_name].map(mapping)\n",
    "    \n",
    "    # Create sine and cosine features for cyclic encoding\n",
    "    df[column_name + '_sin'] = np.sin(2 * np.pi * df[column_name] / 7)\n",
    "    df[column_name + '_cos'] = np.cos(2 * np.pi * df[column_name] / 7)\n",
    "    \n",
    "    # Drop the original day column if you no longer need it\n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mask_column(df, column, mask_value):\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    mask_column_name = f\"{column}_unknown\"\n",
    "    \n",
    "    # Use pd.isna to handle NaN or None\n",
    "    if pd.isna(mask_value):\n",
    "        df[mask_column_name] = df[column].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "    else:\n",
    "        df[mask_column_name] = df[column].apply(lambda x: 1 if x == mask_value else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_value_mode(df, column, mask_value):\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Create the mask column\n",
    "    if pd.isna(mask_value):\n",
    "        mask = df[column].isna()\n",
    "    else:\n",
    "        mask = df[column] == mask_value\n",
    "\n",
    "    # Compute the mode of the column, excluding NaN\n",
    "    column_mode = df.loc[~mask, column].mode()\n",
    "\n",
    "    if column_mode.empty:\n",
    "        raise ValueError(f\"Cannot compute mode for column '{column}' as it has no valid values.\")\n",
    "\n",
    "    # Replace masked values with the mode\n",
    "    df[column] = df[column].where(~mask, column_mode[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def standardize_columns(data, columns):\n",
    "    scaler = StandardScaler()\n",
    "    for column in columns : \n",
    "        if column in data.columns:\n",
    "            data[column] = scaler.fit_transform(data[[column]])\n",
    "    return data\n",
    "\n",
    "def drop_specific_data(data):\n",
    "    #Drop the default column (20% is unknown, there are 3/41188 yes)\n",
    "    #Drop the Pdays column (96.3% unknown)\n",
    "    #Drop duration, as its for benchmarking not modeling (Supposedly, might want to readd this)\n",
    "\n",
    "    # data = data.drop(columns=\"default\")\n",
    "    data = data.drop(columns=\"pdays\")\n",
    "    # data = data.drop(columns=\"duration\")\n",
    "\n",
    "    #Drop the 5 unknown data entries for now (Might readd later to see if models get more accurate)\n",
    "    # data = data.drop(columns=\"nr.employed\")\n",
    "    # data = data.drop(columns=\"euribor3m\")\n",
    "    # data = data.drop(columns=\"cons.conf.idx\")\n",
    "    # data = data.drop(columns=\"emp.var.rate\")\n",
    "    # data = data.drop(columns=\"cons.price.idx\")\n",
    "    \n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(url=\"https://raw.githubusercontent.com/tuclaure/CSU-CS-345/refs/heads/main/Data/bank-additional/bank-additional/bank-additional.csv\",mask_nans = True):\n",
    "    binary_mapping = {\n",
    "        'yes':1,\n",
    "        'no':0,\n",
    "        }\n",
    "\n",
    "    education_mapping = {\n",
    "        'basic.4y':1,\n",
    "        'basic.6y':2,\n",
    "        'basic.9y':3,\n",
    "        'high.school':4,\n",
    "        'illiterate':0,\n",
    "        'professional.course':5,\n",
    "        'university.degree':6,\n",
    "        'unknown': None\n",
    "        }\n",
    "\n",
    "    days_mapping = {\n",
    "            'sun': 0,\n",
    "            'mon': 1,\n",
    "            'tue': 2,\n",
    "            'wed': 3,\n",
    "            'thu': 4,\n",
    "            'fri': 5,\n",
    "            'sat': 6\n",
    "        }\n",
    "\n",
    "    months_mapping = {\n",
    "            'jan': 0,\n",
    "            'feb': 1,\n",
    "            'mar': 2,\n",
    "            'apr': 3,\n",
    "            'may': 4,\n",
    "            'jun': 5,\n",
    "            'jul': 6,\n",
    "            'aug': 7,\n",
    "            'sep': 8,\n",
    "            'oct': 9,\n",
    "            'nov': 10,\n",
    "            'dec': 11\n",
    "        }\n",
    "\n",
    "    binary_columns = [\"housing\", \"loan\", \"y\"]\n",
    "    categorical_columns = [\"job\",\"marital\",\"default\",\"education\",\"contact\",\"poutcome\", \"day_of_week\", \"month\"]\n",
    "    integer_columns = [\"age\", \"balance\", \"duration\", \"campaign\", \"previous\"]\n",
    "\n",
    "    data = fetch_data(url)\n",
    "    \n",
    "    #Remove Duplicate rows\n",
    "    data = data[~data.duplicated()]\n",
    "\n",
    "    #Drop necissary features\n",
    "    data = drop_specific_data(data)\n",
    "\n",
    "    #Map Binary values to 0,1,NaN\n",
    "    data = map_columns(data, binary_columns, binary_mapping)\n",
    "\n",
    "    #Map education to numeric quantities\n",
    "    # data = map_columns(data, [\"education\"], education_mapping)\n",
    "\n",
    "    #One hot encode categorical data\n",
    "    data = one_hot_encode_columns(data, categorical_columns)\n",
    "\n",
    "    #Perform Cyclical mapping for time data\n",
    "    # data = encode_cyclic_information(data, \"day_of_week\", days_mapping)\n",
    "    # data = encode_cyclic_information(data, \"month\", months_mapping)\n",
    "    \n",
    "    #Standardize Integer Columns\n",
    "    data = standardize_columns(data, integer_columns)\n",
    "    \n",
    "    if mask_nans :\n",
    "        #create a mask for housing, loan, education columns\n",
    "        data = mask_column(data, \"housing\", None)\n",
    "        data = mask_column(data, \"loan\", None)\n",
    "        # data = mask_column(data, \"education\", None)\n",
    "\n",
    "        #Imput Housing and Loan to the mode\n",
    "        data = replace_value_mode(data, \"housing\", None)\n",
    "        data = replace_value_mode(data, \"loan\", None)\n",
    "        # data['education'] = data['education'].fillna(-1)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'education' does not exist in the DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[234], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     compare_unknowns_and_values(data, processed_data)\n\u001b[0;32m     73\u001b[0m     data\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m---> 75\u001b[0m \u001b[43mcompare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[234], line 69\u001b[0m, in \u001b[0;36mcompare_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_data\u001b[39m():\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     data \u001b[38;5;241m=\u001b[39m fetch_data()  \u001b[38;5;66;03m# Fetch original data\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Compare unknowns and value counts\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     compare_unknowns_and_values(data, processed_data)\n",
      "Cell \u001b[1;32mIn[233], line 75\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(url, mask_nans)\u001b[0m\n\u001b[0;32m     73\u001b[0m data \u001b[38;5;241m=\u001b[39m mask_column(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m data \u001b[38;5;241m=\u001b[39m mask_column(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mmask_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meducation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#Imput Housing and Loan to the mode\u001b[39;00m\n\u001b[0;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m replace_value_mode(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[232], line 45\u001b[0m, in \u001b[0;36mmask_column\u001b[1;34m(df, column, mask_value)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_column\u001b[39m(df, column, mask_value):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist in the DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m     mask_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Use pd.isna to handle NaN or None\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Column 'education' does not exist in the DataFrame."
     ]
    }
   ],
   "source": [
    "def get_unknown_percentage(df, dataset_name=\"Data\"):\n",
    "    unknown_percentages = {}\n",
    "    unknown_columns_count = 0\n",
    "\n",
    "    # Loop over each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        # Count the occurrences of 'unknown' in the column\n",
    "        unknown_count = (df[column] == 'unknown').sum()\n",
    "\n",
    "        if column == \"pdays\":\n",
    "            unknown_count += (df[column] == 999).sum()\n",
    "\n",
    "        unknown_count += (df[column].isna()).sum()\n",
    "\n",
    "        # Calculate the percentage of 'unknown' values\n",
    "        total_count = len(df[column])\n",
    "        unknown_percentage = (unknown_count / total_count) * 100\n",
    "\n",
    "        # Store the percentage in the dictionary\n",
    "        unknown_percentages[column] = unknown_percentage\n",
    "        if unknown_percentage > 0:\n",
    "            unknown_columns_count += 1\n",
    "            print(f\"{dataset_name} - {column} unknowns: {unknown_percentage}%\")\n",
    "\n",
    "    if unknown_columns_count == 0:\n",
    "        print(f\"{dataset_name} - No unknowns in the data.\")\n",
    "    return unknown_percentages\n",
    "\n",
    "def count_values(column_name, dataframe, dataset_name=\"Data\"):\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the DataFrame.\")\n",
    "        return None\n",
    "    counts = dataframe[column_name].value_counts()\n",
    "    print(f\"{dataset_name} - Value counts for column '{column_name}':\\n{counts}\")\n",
    "    return counts\n",
    "\n",
    "def compare_unknowns_and_values(data, processed_data):\n",
    "    # Compare unknown percentages in original and processed data\n",
    "    print(\"\\n--- Comparing unknown percentages in original and processed data ---\\n\")\n",
    "    \n",
    "    original_unknowns = get_unknown_percentage(data, \"Original Data\")\n",
    "    processed_unknowns = get_unknown_percentage(processed_data, \"Processed Data\")\n",
    "    \n",
    "    # Compare unknown percentages for each column that exists in both dataframes\n",
    "    for column in data.columns:\n",
    "        if column in processed_data.columns:\n",
    "            original_percentage = original_unknowns.get(column, 0)\n",
    "            processed_percentage = processed_unknowns.get(column, 0)\n",
    "            \n",
    "            print(f\"\\nColumn: {column}\")\n",
    "            print(f\"  Original data unknown percentage: {original_percentage}%\")\n",
    "            print(f\"  Processed data unknown percentage: {processed_percentage}%\")\n",
    "            print(\"  Change in unknowns: {:.2f}%\".format(processed_percentage - original_percentage))\n",
    "        else:\n",
    "            print(f\"\\nColumn '{column}' was dropped in the processed data.\")\n",
    "\n",
    "    # Compare value counts for each column that exists in both dataframes\n",
    "    print(\"\\n--- Comparing value counts for each column ---\")\n",
    "    for column in data.columns:\n",
    "        if column in processed_data.columns:\n",
    "            count_values(column, data, \"Original Data\")\n",
    "            count_values(column, processed_data, \"Processed Data\")\n",
    "        else:\n",
    "            print(f\"\\nColumn '{column}' was dropped in the processed data.\")\n",
    "\n",
    "def compare_data():\n",
    "    # Example usage\n",
    "    data = fetch_data()  # Fetch original data\n",
    "    processed_data = preprocessing()  # Preprocess the data\n",
    "\n",
    "    # Compare unknowns and value counts\n",
    "    compare_unknowns_and_values(data, processed_data)\n",
    "    data.describe()\n",
    "\n",
    "compare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_data(url=\"https://raw.githubusercontent.com/tuclaure/CSU-CS-345/refs/heads/main/Data/bank-additional/bank-additional/bank-additional.csv\"):\n",
    "    \n",
    "    data = preprocessing(url)\n",
    "    data.head()\n",
    "    y = data['y']\n",
    "    X = data.drop(columns=['y'])\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "        \n",
    "def create_under_sampled_data(X_train, y_train, under_strength = 0.2):\n",
    "    under = RandomUnderSampler(sampling_strategy=under_strength, random_state=42)\n",
    "    X_train_under, y_train_under = under.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train_under, y_train_under\n",
    "\n",
    "def create_over_sampled_data(X_train, y_train, over_strength=0.5) :\n",
    "       over = SMOTE(sampling_strategy=over_strength, random_state=42)\n",
    "       X_train_over, y_train_over = over.fit_resample(X_train, y_train)\n",
    "       \n",
    "       return X_train_over, y_train_over\n",
    "\n",
    "def create_hybrid_sampled_data(X_train, y_train, under_strength=0.5, over_strength=0.8):\n",
    "    under = RandomUnderSampler(sampling_strategy=under_strength, random_state=42)\n",
    "    X_under, y_under = under.fit_resample(X_train, y_train)\n",
    "    \n",
    "    over = SMOTE(sampling_strategy=over_strength, random_state=42)\n",
    "    X_hybrid, y_hybrid = over.fit_resample(X_under, y_under)\n",
    "    \n",
    "    return X_hybrid, y_hybrid\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, model, param_grid, label):\n",
    "    \n",
    "    pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=pr_auc_scorer,\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"\\nBest Parameters for {label}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix for {label}:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f\"\\nClassification Report for {label}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"ROC AUC Score for {label}: {roc_auc:.2f}\")\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"Precision-Recall AUC for {label}: {pr_auc:.2f}\")\n",
    "    \n",
    "    return precision, recall, pr_auc\n",
    "\n",
    "def class_weighting(y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    return {cls: weight for cls, weight in zip(classes, class_weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model types to make\n",
    "\n",
    "SVM [Tucker]\n",
    "Linear Regression\n",
    "Logistic Regression\n",
    "Gradient Boosting\n",
    "Random Forest [Ryan]\n",
    "Decision tree\n",
    "Bagging\n",
    "\n",
    "Most likely usefull models\n",
    "Logistic Regression\n",
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'education' does not exist in the DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[237], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m     48\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 50\u001b[0m \u001b[43mcreate_svc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[237], line 9\u001b[0m, in \u001b[0;36mcreate_svc\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_svc\u001b[39m():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Get Data Samples\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_X_y_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     X_train_over, y_train_over \u001b[38;5;241m=\u001b[39m create_over_sampled_data(X_train, y_train)\n\u001b[0;32m     11\u001b[0m     X_train_under, y_train_under \u001b[38;5;241m=\u001b[39m create_under_sampled_data(X_train, y_train)\n",
      "Cell \u001b[1;32mIn[235], line 3\u001b[0m, in \u001b[0;36mget_X_y_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_X_y_data\u001b[39m(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/tuclaure/CSU-CS-345/refs/heads/main/Data/bank-additional/bank-additional/bank-additional.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     data\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      5\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[233], line 75\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(url, mask_nans)\u001b[0m\n\u001b[0;32m     73\u001b[0m data \u001b[38;5;241m=\u001b[39m mask_column(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m data \u001b[38;5;241m=\u001b[39m mask_column(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mmask_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meducation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#Imput Housing and Loan to the mode\u001b[39;00m\n\u001b[0;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m replace_value_mode(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[232], line 45\u001b[0m, in \u001b[0;36mmask_column\u001b[1;34m(df, column, mask_value)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_column\u001b[39m(df, column, mask_value):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist in the DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m     mask_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Use pd.isna to handle NaN or None\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Column 'education' does not exist in the DataFrame."
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "}  \n",
    "\n",
    "def create_svc():\n",
    "    #Get Data Samples\n",
    "    X_train, y_train, X_test, y_test = get_X_y_data()\n",
    "    X_train_over, y_train_over = create_over_sampled_data(X_train, y_train)\n",
    "    X_train_under, y_train_under = create_under_sampled_data(X_train, y_train)\n",
    "    \n",
    "    #Get class weights\n",
    "    class_weights_dict_s = class_weighting(y_train)\n",
    "    class_weights_dict_o = class_weighting(y_train_over)\n",
    "    class_weights_dict_u = class_weighting(y_train_under)\n",
    "    \n",
    "    # Define the SVM models\n",
    "    kernel = 'rbf'\n",
    "    svm_standard = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm_over = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm_under = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm_standard_weighted = SVC(kernel=kernel, class_weight=class_weights_dict_s, probability=True, random_state=42)\n",
    "    svm_over_weighted = SVC(kernel=kernel, class_weight=class_weights_dict_o, probability=True, random_state=42)\n",
    "    svm_under_weighted = SVC(kernel=kernel, class_weight=class_weights_dict_u, probability=True, random_state=42)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    precision_std, recall_std, pr_auc_std = train_and_evaluate(X_train, y_train, X_test, y_test, svm_standard, param_grid, \"Standard SVM\")\n",
    "    precision_over, recall_over, pr_auc_over = train_and_evaluate(X_train_over, y_train_over, X_test, y_test, svm_over, param_grid, \"Oversampled SVM\")\n",
    "    precision_under, recall_under, pr_auc_under = train_and_evaluate(X_train_under, y_train_under, X_test, y_test, svm_under, param_grid, \"Undersampled SVM\")\n",
    "    precision_std_w, recall_std_w, pr_auc_std_w = train_and_evaluate(X_train, y_train, X_test, y_test, svm_standard_weighted, param_grid, \"Standard Weighted SVM\")\n",
    "    precision_over_w, recall_over_w, pr_auc_over_w = train_and_evaluate(X_train_over, y_train_over, X_test, y_test, svm_over_weighted, param_grid, \"Oversampled Weighted SVM\")\n",
    "    precision_under_w, recall_under_w, pr_auc_under_w = train_and_evaluate(X_train_under, y_train_under, X_test, y_test, svm_under_weighted, param_grid, \"Undersampled Weighted SVM\")\n",
    "\n",
    "    # Plot Precision-Recall curves for comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.plot(recall_std, precision_std, label=f\"Standard SVM (PR AUC = {pr_auc_std:.2f})\", linestyle='--')\n",
    "    plt.plot(recall_over, precision_over, label=f\"Oversampled SVM (PR AUC = {pr_auc_over:.2f})\", linestyle='-.')\n",
    "    plt.plot(recall_under, precision_under, label=f\"Undersampled SVM (PR AUC = {pr_auc_under:.2f})\", linestyle=':')\n",
    "    plt.plot(recall_std_w, precision_std_w, label=f\"Standard Weighted SVM (PR AUC = {pr_auc_std_w:.2f})\", linestyle='--')\n",
    "    plt.plot(recall_over_w, precision_over_w, label=f\"Oversampled Weighted SVM (PR AUC = {pr_auc_over_w:.2f})\", linestyle='-.')\n",
    "    plt.plot(recall_under_w, precision_under_w, label=f\"Undersampled Weighted SVM (PR AUC = {pr_auc_under_w:.2f})\", linestyle=':')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "create_svc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerMixin.fit_transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[229], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m     44\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 46\u001b[0m \u001b[43mcreate_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[229], line 4\u001b[0m, in \u001b[0;36mcreate_logistic_regression\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_logistic_regression\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Get class weights\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m StandardScaler\n\u001b[1;32m----> 4\u001b[0m     X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     class_weights_dict_s \u001b[38;5;241m=\u001b[39m class_weighting(y_train)\n\u001b[0;32m      6\u001b[0m     class_weights_dict_o \u001b[38;5;241m=\u001b[39m class_weighting(y_train_over)\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerMixin.fit_transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "def create_logistic_regression():\n",
    "    # Get class weights\n",
    "    scaler = StandardScaler\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "    class_weights_dict_s = class_weighting(y_train)\n",
    "    class_weights_dict_o = class_weighting(y_train_over)\n",
    "    class_weights_dict_u = class_weighting(y_train_under)\n",
    "    \n",
    "    # Define the Logistic Regression models\n",
    "    lr_standard = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=42)\n",
    "    lr_over = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=42)\n",
    "    lr_under = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=42)\n",
    "    lr_standard_weighted = LogisticRegression(solver='lbfgs', class_weight=class_weights_dict_s, max_iter=1000, random_state=42)\n",
    "    lr_over_weighted = LogisticRegression(solver='lbfgs', class_weight=class_weights_dict_o, max_iter=1000, random_state=42)\n",
    "    lr_under_weighted = LogisticRegression(solver='lbfgs', class_weight=class_weights_dict_u, max_iter=1000, random_state=42)\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models\n",
    "    precision_std, recall_std, pr_auc_std = train_and_evaluate(X_train, y_train, X_test, y_test, lr_standard, param_grid, \"Standard Logistic Regression\")\n",
    "    precision_over, recall_over, pr_auc_over = train_and_evaluate(X_train_over, y_train_over, X_test, y_test, lr_over, param_grid, \"Oversampled Logistic Regression\")\n",
    "    precision_under, recall_under, pr_auc_under = train_and_evaluate(X_train_under, y_train_under, X_test, y_test, lr_under, param_grid, \"Undersampled Logistic Regression\")\n",
    "    precision_std_w, recall_std_w, pr_auc_std_w = train_and_evaluate(X_train, y_train, X_test, y_test, lr_standard_weighted, param_grid, \"Standard Weighted Logistic Regression\")\n",
    "    precision_over_w, recall_over_w, pr_auc_over_w = train_and_evaluate(X_train_over, y_train_over, X_test, y_test, lr_over_weighted, param_grid, \"Oversampled Weighted Logistic Regression\")\n",
    "    precision_under_w, recall_under_w, pr_auc_under_w = train_and_evaluate(X_train_under, y_train_under, X_test, y_test, lr_under_weighted, param_grid, \"Undersampled Weighted Logistic Regression\")\n",
    "\n",
    "    # Plot Precision-Recall curves for comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.plot(recall_std, precision_std, label=f\"Standard Logistic Regression (PR AUC = {pr_auc_std:.2f})\", linestyle='--')\n",
    "    plt.plot(recall_over, precision_over, label=f\"Oversampled Logistic Regression (PR AUC = {pr_auc_over:.2f})\", linestyle='-.')\n",
    "    plt.plot(recall_under, precision_under, label=f\"Undersampled Logistic Regression (PR AUC = {pr_auc_under:.2f})\", linestyle=':')\n",
    "    plt.plot(recall_std_w, precision_std_w, label=f\"Standard Weighted Logistic Regression (PR AUC = {pr_auc_std_w:.2f})\", linestyle='--')\n",
    "    plt.plot(recall_over_w, precision_over_w, label=f\"Oversampled Weighted Logistic Regression (PR AUC = {pr_auc_over_w:.2f})\", linestyle='-.')\n",
    "    plt.plot(recall_under_w, precision_under_w, label=f\"Undersampled Weighted Logistic Regression (PR AUC = {pr_auc_under_w:.2f})\", linestyle=':')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "create_logistic_regression()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
